{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ec3a14-7edd-4bc0-a0e4-c9630e5d23bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\DELL\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.575 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151.txt分类情况:垃圾邮件\n",
      "152.txt分类情况:垃圾邮件\n",
      "153.txt分类情况:垃圾邮件\n",
      "154.txt分类情况:垃圾邮件\n",
      "155.txt分类情况:普通邮件\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from jieba import cut\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7280f8d7-1eab-433c-b785-0a507f5b28b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(filename):\n",
    "    \"\"\"读取文本并过滤无效字符和长度为1的词\"\"\"\n",
    "    words = []\n",
    "    with open(filename, 'r', encoding='utf-8') as fr:\n",
    "        for line in fr:\n",
    "            line = line.strip()\n",
    "            # 过滤无效字符\n",
    "            line = re.sub(r'[.【】0-9、——。，！~\\*]', '', line)\n",
    "            # 使用jieba.cut()方法对文本切词处理\n",
    "            line = cut(line)\n",
    "            # 过滤长度为1的词\n",
    "            line = filter(lambda word: len(word) > 1, line)\n",
    "            words.extend(line)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13cc6926-9c5f-44b2-9798-4898f2d19545",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "def get_top_words(top_num):\n",
    "    \"\"\"遍历邮件建立词库后返回出现次数最多的词\"\"\"\n",
    "    filename_list = ['files/{}.txt'.format(i) for i in range(151)]\n",
    "    # 遍历邮件建立词库\n",
    "    for filename in filename_list:\n",
    "        all_words.append(get_words(filename))\n",
    "    # itertools.chain()把all_words内的所有列表组合成一个列表\n",
    "    # collections.Counter()统计词个数\n",
    "    freq = Counter(chain(*all_words))\n",
    "    return [i[0] for i in freq.most_common(top_num)]\n",
    "\n",
    "top_words = get_top_words(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "932ac6f3-17e4-4c8d-9056-d803c11706bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = []\n",
    "for words in all_words:\n",
    "    '''\n",
    "    words:\n",
    "    ['国际', 'SCI', '期刊', '材料', '结构力学', '工程', '杂志', '国际', 'SCI', '期刊', '先进', '材料科学',\n",
    "    '材料', '工程', '杂志', '国际', 'SCI', '期刊', '图像处理', '模式识别', '人工智能', '工程', '杂志', '国际',\n",
    "    'SCI', '期刊', '数据', '信息', '科学杂志', '国际', 'SCI', '期刊', '机器', '学习', '神经网络', '人工智能',\n",
    "    '杂志', '国际', 'SCI', '期刊', '能源', '环境', '生态', '温度', '管理', '结合', '信息学', '杂志', '期刊',\n",
    "    '网址', '论文', '篇幅', '控制', '以上', '英文', '字数', '以上', '文章', '撰写', '语言', '英语', '论文',\n",
    "    '研究', '内容', '详实', '方法', '正确', '理论性', '实践性', '科学性', '前沿性', '投稿', '初稿', '需要',\n",
    "    '排版', '录用', '提供', '模版', '排版', '写作', '要求', '正规', '期刊', '正规', '操作', '大牛', '出版社',\n",
    "    '期刊', '期刊', '质量', '放心', '检索', '稳定', '邀请函', '推荐', '身边', '老师', '朋友', '打扰', '请谅解']\n",
    "    '''\n",
    "    word_map = list(map(lambda word: words.count(word), top_words))\n",
    "    '''\n",
    "    word_map:\n",
    "    [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
    "    10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n",
    "    0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    '''\n",
    "    vector.append(word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cc12582-bcb0-44ec-8b17-a4d252ab7d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(filename):\n",
    "    \"\"\"对未知邮件分类\"\"\"\n",
    "    # 构建未知邮件的词向量\n",
    "    words = get_words(filename)\n",
    "    current_vector = np.array(\n",
    "        tuple(map(lambda word: words.count(word), top_words)))\n",
    "    # 预测结果\n",
    "    result = model.predict(current_vector.reshape(1, -1))\n",
    "    return '垃圾邮件' if result == 1 else '普通邮件'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac6795fe-36de-46b7-9644-6ef754d5025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151.txt分类情况:垃圾邮件\n",
      "152.txt分类情况:垃圾邮件\n",
      "153.txt分类情况:垃圾邮件\n",
      "154.txt分类情况:垃圾邮件\n",
      "155.txt分类情况:普通邮件\n"
     ]
    }
   ],
   "source": [
    "print('151.txt分类情况:{}'.format(predict('files/151.txt')))\n",
    "print('152.txt分类情况:{}'.format(predict('files/152.txt')))\n",
    "print('153.txt分类情况:{}'.format(predict('files/153.txt')))\n",
    "print('154.txt分类情况:{}'.format(predict('files/154.txt')))\n",
    "print('155.txt分类情况:{}'.format(predict('files/155.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5efe611-57c2-47e3-a3f6-db57b85a49ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train TFIDF权重\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "151.txt分类情况: 垃圾邮件\n",
      "152.txt分类情况: 垃圾邮件\n",
      "153.txt分类情况: 垃圾邮件\n",
      "154.txt分类情况: 垃圾邮件\n",
      "155.txt分类情况: 垃圾邮件\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def load_all_files(file_range):\n",
    "    \"\"\"加载指定范围内的文件数据\"\"\"\n",
    "    all_words = []\n",
    "    for i in file_range:\n",
    "        filename = f'files/{i}.txt'\n",
    "        all_words.append(get_words(filename))\n",
    "    return all_words\n",
    "\n",
    "def extract_features(all_words, top_num, feature_type):\n",
    "    \"\"\"根据特征类型提取特征\"\"\"\n",
    "    if feature_type == 'frequency':\n",
    "        # 高频词特征提取\n",
    "        vocab = Counter(chain(*all_words)).most_common(top_num)\n",
    "        top_words = [word for word, _ in vocab]\n",
    "        features = np.array([[words.count(word) for word in top_words] for words in all_words])\n",
    "        return features, top_words\n",
    "    elif feature_type == 'tfidf':\n",
    "        # TF-IDF特征提取\n",
    "        corpus = [' '.join(words) for words in all_words]\n",
    "        vectorizer = TfidfVectorizer(max_features=top_num)\n",
    "        features = vectorizer.fit_transform(corpus).toarray()\n",
    "        return features, vectorizer\n",
    "    else:\n",
    "        raise ValueError(\"不支持的feature_type，请选择'frequency'或'tfidf'\")\n",
    "\n",
    "# 参数配置\n",
    "FEATURE_TYPE = 'tfidf'  # 可切换为'frequency'或'tfidf'\n",
    "TOP_NUM = 100\n",
    "\n",
    "# 加载训练数据\n",
    "train_data = load_all_files(range(151))\n",
    "train_labels = np.array([1]*127 + [0]*24)\n",
    "\n",
    "# 特征提取\n",
    "if FEATURE_TYPE == 'frequency':\n",
    "    train_features, feature_encoder = extract_features(train_data, TOP_NUM, FEATURE_TYPE)\n",
    "elif FEATURE_TYPE == 'tfidf':\n",
    "    train_features, feature_encoder = extract_features(train_data, TOP_NUM, FEATURE_TYPE)\n",
    "\n",
    "# 训练模型\n",
    "model = MultinomialNB()\n",
    "model.fit(train_features, train_labels)\n",
    "\n",
    "def predict_file(filename, feature_type, encoder):\n",
    "    \"\"\"预测单个文件\"\"\"\n",
    "    words = get_words(filename)\n",
    "    if feature_type == 'frequency':\n",
    "        features = np.array([[words.count(word) for word in encoder]]).reshape(1, -1)\n",
    "    elif feature_type == 'tfidf':\n",
    "        corpus = [' '.join(words)]\n",
    "        features = encoder.transform(corpus).toarray()\n",
    "    return '垃圾邮件' if model.predict(features)[0] == 1 else '普通邮件'\n",
    "\n",
    "# 预测新文件\n",
    "for file_num in range(151, 156):\n",
    "    filename = f'files/{file_num}.txt'\n",
    "    result = predict_file(filename, FEATURE_TYPE, feature_encoder)\n",
    "    print(f'{file_num}.txt分类情况: {result}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3920d73-62aa-4074-8ea4-bd1ada09d537",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

